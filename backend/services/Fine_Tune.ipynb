{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0720c305-9984-4b38-ad17-e2f91349305b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 19 18:25:55 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off |   00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             73W /  300W |       0MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off |   00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   40C    P0             64W /  300W |       0MiB /  81920MiB |      2%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3e96a51-2225-42de-960d-0d45a95b413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Using Device: cuda\n",
      "ðŸ”¹ Loading Tokenizer...\n",
      "ðŸ”¹ Loading Model (FP16)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c317a8578041a39af9c62b4181b939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Applying LoRA...\n",
      "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758\n",
      "ðŸ”¹ Tokenizing Dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2838aa45c6064b56b4edebe4948f4194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Starting Training Loop...\n",
      "\n",
      "--- Epoch 1/2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0053: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:30<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch 2/2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.0041: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:26<00:00,  3.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Saving model to hcl_final_model...\n",
      "âœ… DONE! You can now run the inference script.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
    "OUTPUT_DIR = \"hcl_final_model\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCH_SIZE = 2\n",
    "LR = 2e-4\n",
    "EPOCHS = 2\n",
    "\n",
    "print(f\"ðŸ”¹ Using Device: {DEVICE}\")\n",
    "\n",
    "data = [\n",
    "    {\"text\": \"### Instruction: Correct grammar. ### Input: i want police for matrnity ### Response: I want the policy for maternity leave.\"},\n",
    "    {\"text\": \"### Instruction: Correct grammar. ### Input: screen broke wat do ### Response: My screen is broken, what should I do?\"},\n",
    "    {\"text\": \"### Instruction: Correct grammar. ### Input: salary not credit yestday ### Response: My salary was not credited yesterday.\"},\n",
    "    {\"text\": \"### Instruction: Correct grammar. ### Input: vpn not connectng help ### Response: The VPN is not connecting, I need help.\"},\n",
    "    \n",
    "    {\"text\": \"### Instruction: Summarize strict context. ### Input: (context: What is leave?, data: Leave is 20 days.) ### Response: Leave is 20 days.\"},\n",
    "    {\"text\": \"### Instruction: Summarize strict context. ### Input: (context: Who is CEO?, data: Mr. Smith is the CEO.) ### Response: Mr. Smith is the CEO.\"},\n",
    "    {\"text\": \"### Instruction: Summarize strict context. ### Input: (context: Where is IT?, data: IT is on Floor 2.) ### Response: IT is on Floor 2.\"},\n",
    "    \n",
    "    {\"text\": \"### Instruction: Summarize strict context. ### Input: (context: Who is CEO?, data: The cafeteria serves pizza.) ### Response: knowledge base has no relevant data\"}\n",
    "]\n",
    "\n",
    "train_data = data * 20 \n",
    "\n",
    "print(\"ðŸ”¹ Loading Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(\"ðŸ”¹ Loading Model (FP16)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_cache=False\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(\"ðŸ”¹ Applying LoRA...\")\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    text_list = [t + tokenizer.eos_token for t in examples[\"text\"]]\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        text_list,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    return tokenized\n",
    "\n",
    "print(\"ðŸ”¹ Tokenizing Dataset...\")\n",
    "raw_dataset = Dataset.from_list(train_data)\n",
    "tokenized_dataset = raw_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_dataset.set_format(\"torch\")\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "print(\"ðŸ”¹ Starting Training Loop...\")\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{EPOCHS} ---\")\n",
    "    progress_bar = tqdm(train_dataloader)\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        progress_bar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nðŸ”¹ Saving model to {OUTPUT_DIR}...\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"âœ… DONE! You can now run the inference script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2273a17d-4d41-45ea-9c3a-b8657c28a859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/tarunm_iitp/.local/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in /home/tarunm_iitp/.local/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: peft in /home/tarunm_iitp/.local/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/tarunm_iitp/.local/lib/python3.12/site-packages (0.49.0)\n",
      "Requirement already satisfied: accelerate in /home/tarunm_iitp/.local/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: datasets in /home/tarunm_iitp/.local/lib/python3.12/site-packages (4.3.0)\n",
      "Requirement already satisfied: trl in /home/tarunm_iitp/.local/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: huggingface_hub in /home/tarunm_iitp/.local/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: filelock in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from torch) (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from transformers) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: xxhash in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.6.2)\n",
      "Requirement already satisfied: certifi in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/tarunm_iitp/.local/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/apps/MLDL/DL-CondaPy3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers peft bitsandbytes accelerate datasets trl huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75c308b1-4377-4cee-a587-fb899e6f16c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading mistralai/Mistral-7B-v0.1 in Float16 (No BitsAndBytes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41827671d7774f2c874d74b9314c93c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 7,283,675,136 || trainable%: 0.5758\n",
      "Starting Training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd77765e15748ebaada9a3aeb676179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ecf185a9f164b36b2d439f4dd75c702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e253a27308014a9fb528e400d3517709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.054100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.684500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.989500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.741100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.469000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.212600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.127100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.120100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.105800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.520400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.497100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.087400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.081500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.072200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.064400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.066100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.068200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.071000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.059600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.062400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.055100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.055700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.065200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.050400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.051300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.050300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.051200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.051500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.053300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.052000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving adapters to hcl_final_model...\n",
      "Training Complete. Now run the Merge script.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
    "OUTPUT_DIR = \"hcl_final_model\"\n",
    "\n",
    "data = [\n",
    "    {\"text\": \"### Instruction: Correct grammar. ### Input: i want police for matrnity ### Response: I want the policy for maternity leave.\"},\n",
    "    {\"text\": \"### Instruction: Correct grammar. ### Input: screen broke wat do ### Response: My screen is broken, what should I do?\"},\n",
    "    {\"text\": \"### Instruction: Correct grammar. ### Input: salary not credit yestday ### Response: My salary was not credited yesterday.\"},\n",
    "    {\"text\": \"### Instruction: Summarize strict context. ### Input: (context: What is leave?, data: Leave is 20 days.) ### Response: Leave is 20 days.\"},\n",
    "]\n",
    "train_data = data * 20\n",
    "dataset = Dataset.from_list(train_data)\n",
    "\n",
    "print(f\"Loading {MODEL_ID} in Float16 (No BitsAndBytes)...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16, # Standard precision\n",
    "    device_map=\"auto\",\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"Starting Training...\")\n",
    "training_args = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    max_steps=50,\n",
    "    fp16=True,\n",
    "    group_by_length=True,\n",
    "    report_to=\"none\",\n",
    "    packing=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(f\"Saving adapters to {OUTPUT_DIR}...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"Training Complete. Now run the Merge script.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19d07f8e-2b74-4830-b444-cb4096583190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model for Merging...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9e30eb59a64ceeb7070701b1cbcf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Adapters...\n",
      "Merging weights...\n",
      "Saving full merged model to hcl_merged_complete...\n",
      "DONE! You can now download the 'hcl_merged_complete' folder.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
    "ADAPTER_DIR = \"hcl_final_model\"\n",
    "MERGED_DIR = \"hcl_merged_complete\"\n",
    "\n",
    "print(\"Loading Base Model for Merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Loading Adapters...\")\n",
    "model = PeftModel.from_pretrained(base_model, ADAPTER_DIR)\n",
    "\n",
    "print(\"Merging weights...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "print(f\"Saving full merged model to {MERGED_DIR}...\")\n",
    "model.save_pretrained(MERGED_DIR, safe_serialization=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "tokenizer.save_pretrained(MERGED_DIR)\n",
    "\n",
    "print(\"DONE! You can now download the 'hcl_merged_complete' folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f04644-c8c4-4889-bee7-b03aba5368a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
